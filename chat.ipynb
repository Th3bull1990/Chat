{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc5b0fe5f214acda93a1b2d7f2739c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTNeoXForCausalLM, AutoConfig\n",
    "from accelerate import init_empty_weights\n",
    "import json \n",
    "\n",
    "# with open('device_map.json', 'r') as f:\n",
    "#     device_map = json.load(f)\n",
    "    \n",
    "checkpoint = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "cache_dir='cache'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir)\n",
    "# model = GPTNeoXForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir).half().cuda()\n",
    "# model = GPTNeoXForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir, device_map=device_map)#.half()\n",
    "model = GPTNeoXForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir, device_map=\"auto\").half()\n",
    "# model.tie_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt_neox.embed_in': 0,\n",
       " 'gpt_neox.layers.0': 0,\n",
       " 'gpt_neox.layers.1': 0,\n",
       " 'gpt_neox.layers.2': 0,\n",
       " 'gpt_neox.layers.3': 0,\n",
       " 'gpt_neox.layers.4': 0,\n",
       " 'gpt_neox.layers.5': 0,\n",
       " 'gpt_neox.layers.6': 0,\n",
       " 'gpt_neox.layers.7': 0,\n",
       " 'gpt_neox.layers.8': 1,\n",
       " 'gpt_neox.layers.9': 1,\n",
       " 'gpt_neox.layers.10': 1,\n",
       " 'gpt_neox.layers.11': 1,\n",
       " 'gpt_neox.layers.12': 1,\n",
       " 'gpt_neox.layers.13': 1,\n",
       " 'gpt_neox.layers.14': 1,\n",
       " 'gpt_neox.layers.15': 1,\n",
       " 'gpt_neox.layers.16': 1,\n",
       " 'gpt_neox.layers.17': 1,\n",
       " 'gpt_neox.layers.18': 2,\n",
       " 'gpt_neox.layers.19': 2,\n",
       " 'gpt_neox.layers.20': 2,\n",
       " 'gpt_neox.layers.21': 2,\n",
       " 'gpt_neox.layers.22': 2,\n",
       " 'gpt_neox.layers.23': 2,\n",
       " 'gpt_neox.layers.24': 2,\n",
       " 'gpt_neox.layers.25': 2,\n",
       " 'gpt_neox.layers.26': 2,\n",
       " 'gpt_neox.layers.27': 2,\n",
       " 'gpt_neox.layers.28': 3,\n",
       " 'gpt_neox.layers.29': 3,\n",
       " 'gpt_neox.layers.30': 3,\n",
       " 'gpt_neox.layers.31': 3,\n",
       " 'gpt_neox.layers.32': 3,\n",
       " 'gpt_neox.layers.33': 3,\n",
       " 'gpt_neox.layers.34': 3,\n",
       " 'gpt_neox.layers.35': 3,\n",
       " 'gpt_neox.final_layer_norm': 3,\n",
       " 'embed_out': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save json\n",
    "# import json \n",
    "# with open('device_map.json', 'w') as f:\n",
    "#     json.dump(model.hf_device_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is a meme, and what\\'s the history behind this word?A meme, or cultural joke that spreads virally, is a unit of cultural information that is mimicked and passed on from person to person. It is often humorous and can involve various forms of media such as images and GIFs. The word \"meme\" comes from the Greek word \"mimema\", which means \"to imitate\". The first known use of \"meme\" was in 1955, in a book by Richard Dawkins called \"The Selfish Gene\".'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\", return_tensors=\"pt\")\n",
    "inputs = inputs.to(0)\n",
    "tokens = model.generate(**inputs, do_sample=True, temperature=0.9, max_length=1024)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
